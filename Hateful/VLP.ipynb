{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "sys.path.insert(0, '/home/jupyter/VLP/pythia')\n",
    "sys.path.insert(0, '/home/jupyter/VLP/')\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTrainingLossMask\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from vlp.loader_utils import batch_list_to_batch_tensors\n",
    "import vlp.seq2seq_loader as seq2seq_loader\n",
    "from vlp.seq2seq_loader import truncate_tokens_pair\n",
    "import PIL\n",
    "from vlp.lang_utils import language_eval\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "import pythia.tasks.processors as pythia_proc\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgDummy(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "args = ArgDummy()\n",
    "DATA_ROOT = '/mnt/ssd/data'\n",
    "HATE_PATH = Path('/home/jupyter/mmf_data/datasets/hateful_memes/defaults/')\n",
    "HATE_FEAT_PATH = Path('/home/jupyter/hateful_features/region_feat_gvd_wo_bgd')\n",
    "\n",
    "args['bert_model'] = 'bert-base-cased' #Bert pre-trained model selected\n",
    "args['seed'] = 123 #random seed for initialization\n",
    "args['len_vis_input'] = 100\n",
    "args['max_tgt_length'] = 20#20 #maximum length of target sequence\n",
    "args['region_det_file_prefix'] = 'feat_cls_1000/coco_detection_vg_100dets_gvd_checkpoint_trainval'\n",
    "args['output_dir'] ='tmp'\n",
    "args['drop_prob'] = 0.1\n",
    "args['model_recover_path'] = './checkpoints/vqa2_g2_lr2e-5_batch512_ft_from_s0.75_b0.25/model.19.bin'\n",
    "args['image_root'] = f'{DATA_ROOT}/flickr30k/region_feat_gvd_wo_bgd/'\n",
    "args['region_bbox_file'] =f'{DATA_ROOT}/flickr30k/region_feat_gvd_wo_bgd/flickr30k_detection_vg_thresh0.2_feat_gvd_checkpoint_trainvaltest.h5'\n",
    "args['do_lower_case'] = True\n",
    "args.region_bbox_file = os.path.join(args.image_root, args.region_bbox_file)\n",
    "args.region_det_file_prefix = os.path.join(args.image_root, args.region_det_file_prefix)\n",
    "args.max_seq_length = args.max_tgt_length + args.len_vis_input + 3 # +3 for 2x[SEP] and [CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "tokenizer.max_len = args.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateStem(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vlp):\n",
    "        super(HateStem, self).__init__()\n",
    "        self.vis_embed = vlp.vis_embed #Linear->ReLU->Linear->ReLU->dropout\n",
    "        self.vis_pe_embed = vlp.vis_pe_embed #Linear->ReLU->dropout\n",
    "        self.bert = vlp.bert # pytorch_pretrained_bert.modeling.BertModel\n",
    "        self.len_vis_input = vlp.len_vis_input\n",
    "        \n",
    "    \n",
    "    def forward(self, vis_feats, vis_pe, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        vis_feats = self.vis_embed(vis_feats) # image region features\n",
    "        vis_pe = self.vis_pe_embed(vis_pe) # image region positional encodings\n",
    "\n",
    "        sequence_output, pooled_output = self.bert(vis_feats, vis_pe, input_ids, token_type_ids,\n",
    "            attention_mask, output_all_encoded_layers=False, len_vis_input=self.len_vis_input)\n",
    "        #print(sequence_output.shape, pooled_output.shape)\n",
    "        vqa2_embed = sequence_output[:, 0]*sequence_output[:, self.len_vis_input+1]\n",
    "        return vqa2_embed\n",
    "        #return sequence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head(nf, n_out, lin_ftrs=None, ps=0.5, bn_final=False, lin_first=False, ):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n",
    "    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "    ps = L(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    layers = []\n",
    "    layers = [Flatten()]\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateClassifier(torch.nn.Module):\n",
    "     def __init__(self, stem):\n",
    "        super(HateClassifier, self).__init__()\n",
    "        self.stem = stem\n",
    "        self.classifier = create_head(768,2, ps=.5)\n",
    "     #def forward(self, id, vis_feats, vis_pe, input_ids, token_type_ids, attention_mask):  \n",
    "     def forward(self, params):  \n",
    "        id, vis_feats, vis_pe, input_ids, token_type_ids, attention_mask = params\n",
    "        embs = self.stem(vis_feats, vis_pe, input_ids, token_type_ids, attention_mask)\n",
    "        return self.classifier(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    hate_stem = torch.load('checkpoints/lm_stem20drop.pth')\n",
    "    \n",
    "    return  HateClassifier(hate_stem).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>caption</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>tex_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>A man in a black shirt is looking at the camera .</td>\n",
       "      <td>False</td>\n",
       "      <td>A man in a black shirt is looking at the camera . &lt;meme&gt; its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>don't be afraid to love again everyone is not like your ex</td>\n",
       "      <td>A man in a suit is talking to a woman in a white shirt .</td>\n",
       "      <td>False</td>\n",
       "      <td>A man in a suit is talking to a woman in a white shirt . &lt;meme&gt; don't be afraid to love again everyone is not like your ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>A cat with a red bow tied around its neck sits next to a pillow .</td>\n",
       "      <td>False</td>\n",
       "      <td>A cat with a red bow tied around its neck sits next to a pillow . &lt;meme&gt; putting bows on your pet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png    0.0   \n",
       "1  23058  img/23058.png    0.0   \n",
       "2  13894  img/13894.png    0.0   \n",
       "\n",
       "                                                         text  \\\n",
       "0            its their character not their color that matters   \n",
       "1  don't be afraid to love again everyone is not like your ex   \n",
       "2                                    putting bows on your pet   \n",
       "\n",
       "                                                             caption is_valid  \\\n",
       "0                  A man in a black shirt is looking at the camera .    False   \n",
       "1           A man in a suit is talking to a woman in a white shirt .    False   \n",
       "2  A cat with a red bow tied around its neck sits next to a pillow .    False   \n",
       "\n",
       "                                                                                                                      tex_cap  \n",
       "0                   A man in a black shirt is looking at the camera . <meme> its their character not their color that matters  \n",
       "1  A man in a suit is talking to a woman in a white shirt . <meme> don't be afraid to love again everyone is not like your ex  \n",
       "2                           A cat with a red bow tied around its neck sits next to a pillow . <meme> putting bows on your pet  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =  Path('/home/jupyter/VLP')\n",
    "data =pd.read_csv(path/'captioned.csv')\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLPID(TensorBase):pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_row(row, proc, q=None):\n",
    "    img_id = row.id\n",
    "    text = row.text\n",
    "    if q is None: q = text\n",
    "    img_file = id_to_img_path(img_id)\n",
    "    tokens = tokenizer.tokenize(q)\n",
    "    if len(tokens) > args['max_tgt_length']:\n",
    "        idx = randint(0, len(tokens) - args['max_tgt_length'])\n",
    "        tokens = tokens[idx:idx+args['max_tgt_length']]\n",
    "        assert len(tokens) == args['max_tgt_length']\n",
    "    instance = (img_file, tokens, {'answers': ['dummy']})\n",
    "    input_ids, segment_ids, input_mask, conv_feats, vis_masked_pos, vis_pe= proc(instance)\n",
    "    return VLPID(img_id), conv_feats, vis_pe, input_ids, segment_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_text(id):\n",
    "    return first(data[data.id==id].text)\n",
    "def id_to_label(id):\n",
    "    return first(data[data.id==id].label)\n",
    "def id_to_img_path(id):\n",
    "    id = f'{int(id):05d}'\n",
    "    return str(HATE_PATH/f'images/img/{id}.png')\n",
    "def id_to_features(id):\n",
    "    id_suf = id[-2:]\n",
    "    feat = HATE_FEAT_PATH/'feat_cls_1000'/f'hateful_vlp_checkpoint_trainval_feat{id_suf}.h5'\n",
    "    clas = HATE_FEAT_PATH/'feat_cls_1000'/f'hateful_vlp_checkpoint_trainval_cls{id_suf}.h5'\n",
    "    bbox = HATE_FEAT_PATH/'raw_bbox'/f'hateful_vlp_checkpoint_trainval_bbox{id_suf}.h5'\n",
    "    return feat, clas, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(idx, proc, q=None):\n",
    "    return load_from_row(data.iloc[idx],proc, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, shuffle, choices\n",
    "from random import random as rand\n",
    "import h5py\n",
    "from vlp.loader_utils import get_random_word\n",
    "class PreprocessVLP(Pipeline):\n",
    "    \"\"\" Pre-processing steps for pretraining transformer \"\"\"\n",
    "\n",
    "    def __init__(self, max_pred, mask_prob, vocab_words, indexer, max_len=512, block_mask=False, truncate_config={}, mask_image_regions=False, mode=\"s2s\", len_vis_input=49, vis_mask_prob=0.25, \n",
    "                  region_bbox_prefix='',  region_bbox_file = None, region_det_file_prefix='', local_rank=-1, load_vqa_ann=False, id_digits=3):\n",
    "        super().__init__()\n",
    "        self.max_pred = max_pred  # max tokens of prediction\n",
    "        self.mask_prob = mask_prob  # masking probability\n",
    "        self.vocab_words = vocab_words  # vocabulary (sub)words\n",
    "        self.indexer = indexer  # function from token to token index\n",
    "        self.max_len = max_len\n",
    "        self._tril_matrix = torch.tril(torch.ones(\n",
    "            (max_len, max_len), dtype=torch.long))\n",
    "        self.always_truncate_tail = truncate_config.get(\n",
    "            'always_truncate_tail', False)\n",
    "        self.max_len_b = truncate_config.get('max_len_b', None)\n",
    "        self.trunc_seg = truncate_config.get('trunc_seg', None)\n",
    "        self.mask_image_regions = mask_image_regions\n",
    "        assert mode in (\"s2s\", \"bi\")\n",
    "        self.mode = mode\n",
    "        self.region_bbox_prefix = region_bbox_prefix\n",
    "        self.region_bbox_file = region_bbox_file\n",
    "        self.region_det_file_prefix = region_det_file_prefix\n",
    "        self.id_digits = id_digits\n",
    "\n",
    "\n",
    "        self.len_vis_input = len_vis_input\n",
    "        self.vis_mask_prob = vis_mask_prob\n",
    "\n",
    "        # for images\n",
    "        if load_vqa_ann:\n",
    "            # import packages from pythia\n",
    "            import pythia.tasks.processors as pythia_proc # VQAAnswerProcessor\n",
    "            from pythia.utils.configuration import ConfigNode\n",
    "            args = {'vocab_file': '/home/jupyter/VLP/pythia/data/vocabs/answers_vqa.txt', 'num_answers':10, 'preprocessor':{'type':'simple_word', 'params':{}}}\n",
    "            args = ConfigNode(args)\n",
    "            self.ans_proc = pythia_proc.registry.get_processor_class('vqa_answer')(args)\n",
    "        else:\n",
    "            self.ans_proc = None\n",
    "\n",
    "\n",
    "    def __call__(self, instance):\n",
    "        img_path, tokens_b = instance[:2]\n",
    "        tokens_a = ['[UNK]'] * self.len_vis_input\n",
    "\n",
    "        truncate_tokens_pair(tokens_a, tokens_b,\n",
    "            self.len_vis_input + self.max_len_b, max_len_b=self.max_len_b,\n",
    "            trunc_seg=self.trunc_seg, always_truncate_tail=self.always_truncate_tail)\n",
    "\n",
    "        # Add Special Tokens\n",
    "        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "\n",
    "        segment_ids = [0] * (len(tokens_a)+2) + [1] * (len(tokens_b)+1)\n",
    "       \n",
    "\n",
    "        # For masked Language Models\n",
    "        # the number of prediction is sometimes less than max_pred when sequence is short\n",
    "        effective_length = len(tokens_b)\n",
    "        n_pred = min(self.max_pred, max(\n",
    "            1, int(round(effective_length * self.mask_prob))))\n",
    "        # candidate positions of masked tokens\n",
    "        cand_pos = []\n",
    "        special_pos = set()\n",
    "        for i, tk in enumerate(tokens):\n",
    "            # only mask tokens_b (target sequence)\n",
    "            # we will mask [SEP] as an ending symbol\n",
    "            if (i >= len(tokens_a)+2) and (tk != '[CLS]'):\n",
    "                cand_pos.append(i)\n",
    "            else:\n",
    "                special_pos.add(i)\n",
    "        shuffle(cand_pos)\n",
    "\n",
    "        masked_pos = cand_pos[:n_pred]\n",
    "        if self.mask_image_regions:\n",
    "            vis_masked_pos = np.random.choice(self.len_vis_input,\n",
    "                int(self.len_vis_input*self.vis_mask_prob), replace=False)+1 # +1 for [CLS], always of the same length, no need to pad\n",
    "        else:\n",
    "            vis_masked_pos = []\n",
    "\n",
    "        masked_tokens = [tokens[pos] for pos in masked_pos]\n",
    "        for pos in masked_pos:\n",
    "            if rand() < 0.8:  # 80%\n",
    "                tokens[pos] = '[MASK]'\n",
    "            elif rand() < 0.5:  # 10%\n",
    "                tokens[pos] = get_random_word(self.vocab_words)\n",
    "        # Token Indexing\n",
    "        input_ids = self.indexer(tokens)\n",
    "\n",
    "        # Zero Padding\n",
    "        n_pad = self.max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "        # self-attention mask\n",
    "        input_mask = torch.zeros(self.max_len, self.max_len, dtype=torch.long)\n",
    "        second_st, second_end = len(tokens_a)+2, len(tokens_a)+len(tokens_b)+3\n",
    "\n",
    "        if self.mode == \"s2s\":\n",
    "            input_mask[:, :len(tokens_a)+2].fill_(1)\n",
    "            input_mask[second_st:second_end, second_st:second_end].copy_(\n",
    "                self._tril_matrix[:second_end-second_st, :second_end-second_st])\n",
    "        else:\n",
    "            input_mask = torch.tensor([1] * len(tokens) + [0] * n_pad, dtype=torch.long) \\\n",
    "                .unsqueeze(0).expand(self.max_len, self.max_len).clone()\n",
    "\n",
    "        if self.mask_image_regions:\n",
    "            input_mask[:, vis_masked_pos].fill_(0) # block the masked visual feature\n",
    "\n",
    "        # Zero Padding for masked target\n",
    "        if self.max_pred > n_pred:\n",
    "            n_pad = self.max_pred - n_pred\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "\n",
    "        # loading pre-processed features\n",
    "        img_id = img_path.split('/')[-1].split('.')[0]\n",
    "        if self.region_bbox_file is not None:\n",
    "            bbox_file = self.region_bbox_file\n",
    "        else:\n",
    "            bbox_file = self.region_bbox_prefix+'_bbox'+img_id[-self.id_digits:]+'.h5'\n",
    "        if self.region_det_file_prefix != '':\n",
    "            # read data from h5 files\n",
    "            with h5py.File(self.region_det_file_prefix+'_feat'+img_id[-self.id_digits:] +'.h5', 'r') as region_feat_f, \\\n",
    "                    h5py.File(self.region_det_file_prefix+'_cls'+img_id[-self.id_digits:] +'.h5', 'r') as region_cls_f, \\\n",
    "                    h5py.File(bbox_file, 'r') as region_bbox_f:\n",
    "                img = torch.from_numpy(region_feat_f[img_id][:]).float()\n",
    "                cls_label = torch.from_numpy(region_cls_f[img_id][:]).float()\n",
    "                vis_pe = torch.from_numpy(region_bbox_f[img_id][:])\n",
    "        else:\n",
    "            # legacy, for some datasets, read data from numpy files\n",
    "            img = torch.from_numpy(np.load(img_path))\n",
    "            cls_label = torch.from_numpy(np.load(img_path.replace('.npy', '_cls_prob.npy')))\n",
    "            with h5py.File(self.region_bbox_file, 'r') as region_bbox_f:\n",
    "                vis_pe = torch.from_numpy(region_bbox_f[img_id][:])\n",
    "\n",
    "        # lazy normalization of the coordinates...\n",
    "        w_est = torch.max(vis_pe[:, [0, 2]])*1.+1e-5\n",
    "        h_est = torch.max(vis_pe[:, [1, 3]])*1.+1e-5\n",
    "        vis_pe[:, [0, 2]] /= w_est\n",
    "        vis_pe[:, [1, 3]] /= h_est\n",
    "        assert h_est > 0, 'should greater than 0! {}'.format(h_est)\n",
    "        assert w_est > 0, 'should greater than 0! {}'.format(w_est)\n",
    "        rel_area = (vis_pe[:, 3]-vis_pe[:, 1])*(vis_pe[:, 2]-vis_pe[:, 0])\n",
    "        rel_area.clamp_(0)\n",
    "\n",
    "        vis_pe = torch.cat((vis_pe[:, :4], rel_area.view(-1, 1), vis_pe[:, 5:]), -1) # confident score\n",
    "        normalized_coord = F.normalize(vis_pe.data[:, :5]-0.5, dim=-1)\n",
    "        vis_pe = torch.cat((F.layer_norm(vis_pe, [6]), \\\n",
    "            F.layer_norm(cls_label, [1601])), dim=-1) # 1601 hard coded...\n",
    "\n",
    "        \n",
    "        return (input_ids, segment_ids, input_mask, img, vis_masked_pos, vis_pe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pref = HATE_FEAT_PATH / 'feat_cls_1000/hateful_vlp_checkpoint_trainval'\n",
    "bbox_pref = HATE_FEAT_PATH / 'raw_bbox/hateful_vlp_checkpoint_trainval'\n",
    "id_digits=2\n",
    "\n",
    "truncate_config={\n",
    "    'max_len_b': args.max_tgt_length, 'trunc_seg': 'b', 'always_truncate_tail': True}\n",
    "\n",
    "max_masked = 10\n",
    "mask_prob = .2\n",
    "mask_img=True\n",
    "vis_mask_prob = 0.2\n",
    "train_proc = PreprocessVLP(max_masked, mask_prob,\n",
    "    list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, args.max_seq_length,\n",
    "    truncate_config=truncate_config,mask_image_regions=mask_img, vis_mask_prob=vis_mask_prob,\n",
    "    mode=\"bi\", len_vis_input=args.len_vis_input, \n",
    "    region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "    load_vqa_ann=True)\n",
    "\n",
    "val_proc = PreprocessVLP(0, 0,\n",
    "    list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, args.max_seq_length,\n",
    "    truncate_config=truncate_config,mask_image_regions=mask_img, vis_mask_prob=vis_mask_prob,\n",
    "    mode=\"bi\", len_vis_input=args.len_vis_input, \n",
    "    region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "    load_vqa_ann=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_y(row):\n",
    "    return int(row['label'])\n",
    "\n",
    "class LoadRow(Transform):\n",
    "   \n",
    "    def __init__(self, split_idx, processor):\n",
    "        self.split_idx = split_idx\n",
    "        self.proc = processor\n",
    "    \n",
    "    def encodes(self, x, **kwargs):\n",
    "        #print(self.proc.mask_prob)\n",
    "        return load_from_row(x, self.proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 500)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DataBlock(get_x = Pipeline([LoadRow(0, train_proc),LoadRow(1, val_proc)]), get_y=get_y,  splitter=ColSplitter('is_valid'))\n",
    "dls = db.dataloaders(data[:9000],bs=32, device=device, decode = lambda x:34)\n",
    "\n",
    "\n",
    "len(dls.train_ds), len(dls.valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typedispatch\n",
    "def show_batch(x:tuple, y, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    ids = x[0]\n",
    "    for i,ctx in enumerate(ctxs):\n",
    "        show(ids[i], ctx)\n",
    "    return ctxs\n",
    "\n",
    "def show(id, ctx, **kwargs):\n",
    "        id = id.item()\n",
    "        tit = str(id_to_label(id)) + ': ' + id_to_text(id)\n",
    "        show_title(tit[:20], ctx =ctx)\n",
    "        show_image(PILImage.create(id_to_img_path(id)), ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls.show_batch(dls.one_batch(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vlp_splitter(model):\n",
    "    return L(params(model.stem.vis_embed) + params(model.stem.vis_pe_embed), \n",
    "            params(model.stem.bert),\n",
    "            params(model.classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model()\n",
    "learn = Learner(dls, model,nn.CrossEntropyLoss(),metrics=[accuracy, RocAucBinary()], splitter=vlp_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.758101</td>\n",
       "      <td>0.712327</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.663679</td>\n",
       "      <td>0.770260</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.667920</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604397</td>\n",
       "      <td>0.698316</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.699552</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.527745</td>\n",
       "      <td>0.683571</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.727840</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.455180</td>\n",
       "      <td>0.753706</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.729008</td>\n",
       "      <td>02:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.408469</td>\n",
       "      <td>0.782015</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.728720</td>\n",
       "      <td>02:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.334392</td>\n",
       "      <td>0.831035</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.730160</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.290955</td>\n",
       "      <td>0.855964</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.726688</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(8, lr_max=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/attempt21.pth')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('attempt21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dl_experiments/Hateful/util.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = F.softmax(preds)\n"
     ]
    }
   ],
   "source": [
    "gen_submit(learn, 'attempts/attempt21.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "vocab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f05fd3d40420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationInterpretation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_classification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/fastai/interpret.py\u001b[0m in \u001b[0;36mfrom_learner\u001b[0;34m(cls, learn, ds_idx, dl, act)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"Construct interpretation object from a learner\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_decoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtop_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/fastai/interpret.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dl, inputs, preds, targs, decoded, losses)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_attr_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgather_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgather_attr_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/fastcore/transform.py\u001b[0m in \u001b[0;36mgather_attrs\u001b[0;34m(o, k, nm)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrgot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: vocab"
     ]
    }
   ],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_submit(learn, 'attempts/attempt12.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlp",
   "language": "python",
   "name": "vlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
