{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "sys.path.insert(0, '/home/jupyter/VLP/pythia')\n",
    "sys.path.insert(0, '/home/jupyter/VLP/')\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTrainingLossMask\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from vlp.loader_utils import batch_list_to_batch_tensors\n",
    "import vlp.seq2seq_loader as seq2seq_loader\n",
    "from vlp.seq2seq_loader import truncate_tokens_pair\n",
    "import PIL\n",
    "from vlp.lang_utils import language_eval\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "import pythia.tasks.processors as pythia_proc\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgDummy(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "args = ArgDummy()\n",
    "DATA_ROOT = '/mnt/ssd/data'\n",
    "HATE_PATH = Path('/home/jupyter/mmf_data/datasets/hateful_memes/defaults/')\n",
    "HATE_FEAT_PATH = Path('/home/jupyter/hateful_features/region_feat_gvd_wo_bgd')\n",
    "\n",
    "args['bert_model'] = 'bert-base-cased' #Bert pre-trained model selected\n",
    "args['seed'] = 123 #random seed for initialization\n",
    "args['len_vis_input'] = 100\n",
    "args['max_tgt_length'] = 20 #maximum length of target sequence\n",
    "args['region_det_file_prefix'] = 'feat_cls_1000/coco_detection_vg_100dets_gvd_checkpoint_trainval'\n",
    "args['output_dir'] ='tmp'\n",
    "args['drop_prob'] = 0.1\n",
    "args['model_recover_path'] = './checkpoints/vqa2_g2_lr2e-5_batch512_ft_from_s0.75_b0.25/model.19.bin'\n",
    "args['image_root'] = f'{DATA_ROOT}/flickr30k/region_feat_gvd_wo_bgd/'\n",
    "args['region_bbox_file'] =f'{DATA_ROOT}/flickr30k/region_feat_gvd_wo_bgd/flickr30k_detection_vg_thresh0.2_feat_gvd_checkpoint_trainvaltest.h5'\n",
    "args['do_lower_case'] = True\n",
    "args.region_bbox_file = os.path.join(args.image_root, args.region_bbox_file)\n",
    "args.region_det_file_prefix = os.path.join(args.image_root, args.region_det_file_prefix)\n",
    "args.max_seq_length = args.max_tgt_length + args.len_vis_input + 3 # +3 for 2x[SEP] and [CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "tokenizer.max_len = args.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateStem(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vlp):\n",
    "        super(HateStem, self).__init__()\n",
    "        self.vis_embed = vlp.vis_embed #Linear->ReLU->Linear->ReLU->dropout\n",
    "        self.vis_pe_embed = vlp.vis_pe_embed #Linear->ReLU->dropout\n",
    "        self.bert = vlp.bert # pytorch_pretrained_bert.modeling.BertModel\n",
    "        self.len_vis_input = vlp.len_vis_input\n",
    "        \n",
    "    \n",
    "    def forward(self, vis_feats, vis_pe, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        vis_feats = self.vis_embed(vis_feats) # image region features\n",
    "        vis_pe = self.vis_pe_embed(vis_pe) # image region positional encodings\n",
    "\n",
    "        sequence_output, pooled_output = self.bert(vis_feats, vis_pe, input_ids, token_type_ids,\n",
    "            attention_mask, output_all_encoded_layers=False, len_vis_input=self.len_vis_input)\n",
    "        #print(sequence_output.shape, pooled_output.shape)\n",
    "        vqa2_embed = sequence_output[:, 0]*sequence_output[:, self.len_vis_input+1]\n",
    "        return vqa2_embed\n",
    "        #return sequence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head(nf, n_out, lin_ftrs=None, ps=0.5, bn_final=False, lin_first=False, ):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n",
    "    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "    ps = L(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    layers = []\n",
    "    layers = [Flatten()]\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLPBatch():\n",
    "    def __init__(self, id, vis_feats, vis_pe, input_ids, token_type_ids, attention_mask):\n",
    "        \n",
    "        store_attr(self, 'id, vis_feats, vis_pe, input_ids, token_type_ids, attention_mask')\n",
    "    \n",
    "    @classmethod    \n",
    "    def merge(cls, batches):\n",
    "        data = []\n",
    "        LB = L(batches)\n",
    "        for attr in 'id vis_feats vis_pe input_ids token_type_ids attention_mask'.split():\n",
    "            to_stack = list(LB.map(attrgetter(attr)))\n",
    "            data.append(torch.stack(to_stack))\n",
    "        return cls(*data)\n",
    "    \n",
    "    def to_device(self, device):\n",
    "        for attr in 'id vis_feats vis_pe input_ids token_type_ids attention_mask'.split():\n",
    "            setattr(self, attr, getattr(self, attr).to(device))\n",
    "        return self\n",
    "    def __repr__(self):\n",
    "        return f'row {self.id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateClassifier(torch.nn.Module):\n",
    "     def __init__(self, stem):\n",
    "        super(HateClassifier, self).__init__()\n",
    "        self.stem = stem\n",
    "        self.classifier = create_head(768,2)\n",
    "     def forward(self, vlp_batch):  \n",
    "        embs = self.stem(vlp_batch.vis_feats, vlp_batch.vis_pe, vlp_batch.input_ids, vlp_batch.token_type_ids, vlp_batch.attention_mask)\n",
    "        return self.classifier(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    hate_stem = torch.load('checkpoints/vlp_stem.pth')\n",
    "    \n",
    "    return  HateClassifier(hate_stem).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>caption</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>tex_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>A man in a black shirt is looking at the camera .</td>\n",
       "      <td>False</td>\n",
       "      <td>A man in a black shirt is looking at the camera . &lt;meme&gt; its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>don't be afraid to love again everyone is not like your ex</td>\n",
       "      <td>A man in a suit is talking to a woman in a white shirt .</td>\n",
       "      <td>False</td>\n",
       "      <td>A man in a suit is talking to a woman in a white shirt . &lt;meme&gt; don't be afraid to love again everyone is not like your ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>A cat with a red bow tied around its neck sits next to a pillow .</td>\n",
       "      <td>False</td>\n",
       "      <td>A cat with a red bow tied around its neck sits next to a pillow . &lt;meme&gt; putting bows on your pet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png    0.0   \n",
       "1  23058  img/23058.png    0.0   \n",
       "2  13894  img/13894.png    0.0   \n",
       "\n",
       "                                                         text  \\\n",
       "0            its their character not their color that matters   \n",
       "1  don't be afraid to love again everyone is not like your ex   \n",
       "2                                    putting bows on your pet   \n",
       "\n",
       "                                                             caption is_valid  \\\n",
       "0                  A man in a black shirt is looking at the camera .    False   \n",
       "1           A man in a suit is talking to a woman in a white shirt .    False   \n",
       "2  A cat with a red bow tied around its neck sits next to a pillow .    False   \n",
       "\n",
       "                                                                                                                      tex_cap  \n",
       "0                   A man in a black shirt is looking at the camera . <meme> its their character not their color that matters  \n",
       "1  A man in a suit is talking to a woman in a white shirt . <meme> don't be afraid to love again everyone is not like your ex  \n",
       "2                           A cat with a red bow tied around its neck sits next to a pillow . <meme> putting bows on your pet  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =  Path('/home/jupyter/VLP')\n",
    "data =pd.read_csv(path/'captioned.csv')\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_row(row, proc, q=None):\n",
    "    img_id = row.id\n",
    "    text = row.text\n",
    "    if q is None: q = text\n",
    "    img_file = id_to_img_path(img_id)\n",
    "    instance = (img_file, tokenizer.tokenize(q), {'answers': ['dummy']})\n",
    "    input_ids, segment_ids, input_mask, conv_feats, vis_masked_pos, vis_pe= proc(instance)\n",
    "    return VLPBatch(torch.tensor(img_id), conv_feats, vis_pe, input_ids, segment_ids, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_img_path(id):\n",
    "    id = f'{int(id):05d}'\n",
    "    return str(HATE_PATH/f'images/img/{id}.png')\n",
    "def id_to_features(id):\n",
    "    id_suf = id[-2:]\n",
    "    feat = HATE_FEAT_PATH/'feat_cls_1000'/f'hateful_vlp_checkpoint_trainval_feat{id_suf}.h5'\n",
    "    clas = HATE_FEAT_PATH/'feat_cls_1000'/f'hateful_vlp_checkpoint_trainval_cls{id_suf}.h5'\n",
    "    bbox = HATE_FEAT_PATH/'raw_bbox'/f'hateful_vlp_checkpoint_trainval_bbox{id_suf}.h5'\n",
    "    return feat, clas, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(idx, proc, q=None):\n",
    "    return load_from_row(data.iloc[idx],proc, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, shuffle, choices\n",
    "from random import random as rand\n",
    "import h5py\n",
    "from vlp.loader_utils import get_random_word\n",
    "class PreprocessVLP(Pipeline):\n",
    "    \"\"\" Pre-processing steps for pretraining transformer \"\"\"\n",
    "\n",
    "    def __init__(self, max_pred, mask_prob, vocab_words, indexer, max_len=512, block_mask=False, truncate_config={}, mask_image_regions=False, mode=\"s2s\", len_vis_input=49, vis_mask_prob=0.25, \n",
    "                  region_bbox_prefix='',  region_bbox_file = None, region_det_file_prefix='', local_rank=-1, load_vqa_ann=False, id_digits=3):\n",
    "        super().__init__()\n",
    "        self.max_pred = max_pred  # max tokens of prediction\n",
    "        self.mask_prob = mask_prob  # masking probability\n",
    "        self.vocab_words = vocab_words  # vocabulary (sub)words\n",
    "        self.indexer = indexer  # function from token to token index\n",
    "        self.max_len = max_len\n",
    "        self._tril_matrix = torch.tril(torch.ones(\n",
    "            (max_len, max_len), dtype=torch.long))\n",
    "        self.always_truncate_tail = truncate_config.get(\n",
    "            'always_truncate_tail', False)\n",
    "        self.max_len_b = truncate_config.get('max_len_b', None)\n",
    "        self.trunc_seg = truncate_config.get('trunc_seg', None)\n",
    "        self.mask_image_regions = mask_image_regions\n",
    "        assert mode in (\"s2s\", \"bi\")\n",
    "        self.mode = mode\n",
    "        self.region_bbox_prefix = region_bbox_prefix\n",
    "        self.region_bbox_file = region_bbox_file\n",
    "        self.region_det_file_prefix = region_det_file_prefix\n",
    "        self.id_digits = id_digits\n",
    "\n",
    "\n",
    "        self.len_vis_input = len_vis_input\n",
    "        self.vis_mask_prob = vis_mask_prob\n",
    "\n",
    "        # for images\n",
    "        if load_vqa_ann:\n",
    "            # import packages from pythia\n",
    "            import pythia.tasks.processors as pythia_proc # VQAAnswerProcessor\n",
    "            from pythia.utils.configuration import ConfigNode\n",
    "            args = {'vocab_file': '/home/jupyter/VLP/pythia/data/vocabs/answers_vqa.txt', 'num_answers':10, 'preprocessor':{'type':'simple_word', 'params':{}}}\n",
    "            args = ConfigNode(args)\n",
    "            self.ans_proc = pythia_proc.registry.get_processor_class('vqa_answer')(args)\n",
    "        else:\n",
    "            self.ans_proc = None\n",
    "\n",
    "\n",
    "    def __call__(self, instance):\n",
    "        img_path, tokens_b = instance[:2]\n",
    "        tokens_a = ['[UNK]'] * self.len_vis_input\n",
    "\n",
    "        truncate_tokens_pair(tokens_a, tokens_b,\n",
    "            self.len_vis_input + self.max_len_b, max_len_b=self.max_len_b,\n",
    "            trunc_seg=self.trunc_seg, always_truncate_tail=self.always_truncate_tail)\n",
    "\n",
    "        # Add Special Tokens\n",
    "        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "\n",
    "        segment_ids = [0] * (len(tokens_a)+2) + [1] * (len(tokens_b)+1)\n",
    "       \n",
    "\n",
    "        # For masked Language Models\n",
    "        # the number of prediction is sometimes less than max_pred when sequence is short\n",
    "        effective_length = len(tokens_b)\n",
    "        n_pred = min(self.max_pred, max(\n",
    "            1, int(round(effective_length * self.mask_prob))))\n",
    "        # candidate positions of masked tokens\n",
    "        cand_pos = []\n",
    "        special_pos = set()\n",
    "        for i, tk in enumerate(tokens):\n",
    "            # only mask tokens_b (target sequence)\n",
    "            # we will mask [SEP] as an ending symbol\n",
    "            if (i >= len(tokens_a)+2) and (tk != '[CLS]'):\n",
    "                cand_pos.append(i)\n",
    "            else:\n",
    "                special_pos.add(i)\n",
    "        shuffle(cand_pos)\n",
    "\n",
    "        masked_pos = cand_pos[:n_pred]\n",
    "        if self.mask_image_regions:\n",
    "            vis_masked_pos = np.random.choice(self.len_vis_input,\n",
    "                int(self.len_vis_input*self.vis_mask_prob), replace=False)+1 # +1 for [CLS], always of the same length, no need to pad\n",
    "        else:\n",
    "            vis_masked_pos = []\n",
    "\n",
    "        masked_tokens = [tokens[pos] for pos in masked_pos]\n",
    "        for pos in masked_pos:\n",
    "            if rand() < 0.8:  # 80%\n",
    "                tokens[pos] = '[MASK]'\n",
    "            elif rand() < 0.5:  # 10%\n",
    "                tokens[pos] = get_random_word(self.vocab_words)\n",
    "        # Token Indexing\n",
    "        input_ids = self.indexer(tokens)\n",
    "\n",
    "        # Zero Padding\n",
    "        n_pad = self.max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "        # self-attention mask\n",
    "        input_mask = torch.zeros(self.max_len, self.max_len, dtype=torch.long)\n",
    "        second_st, second_end = len(tokens_a)+2, len(tokens_a)+len(tokens_b)+3\n",
    "\n",
    "        if self.mode == \"s2s\":\n",
    "            input_mask[:, :len(tokens_a)+2].fill_(1)\n",
    "            input_mask[second_st:second_end, second_st:second_end].copy_(\n",
    "                self._tril_matrix[:second_end-second_st, :second_end-second_st])\n",
    "        else:\n",
    "            input_mask = torch.tensor([1] * len(tokens) + [0] * n_pad, dtype=torch.long) \\\n",
    "                .unsqueeze(0).expand(self.max_len, self.max_len).clone()\n",
    "\n",
    "        if self.mask_image_regions:\n",
    "            input_mask[:, vis_masked_pos].fill_(0) # block the masked visual feature\n",
    "\n",
    "        # Zero Padding for masked target\n",
    "        if self.max_pred > n_pred:\n",
    "            n_pad = self.max_pred - n_pred\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "\n",
    "        # loading pre-processed features\n",
    "        img_id = img_path.split('/')[-1].split('.')[0]\n",
    "        if self.region_bbox_file is not None:\n",
    "            bbox_file = self.region_bbox_file\n",
    "        else:\n",
    "            bbox_file = self.region_bbox_prefix+'_bbox'+img_id[-self.id_digits:]+'.h5'\n",
    "        if self.region_det_file_prefix != '':\n",
    "            # read data from h5 files\n",
    "            with h5py.File(self.region_det_file_prefix+'_feat'+img_id[-self.id_digits:] +'.h5', 'r') as region_feat_f, \\\n",
    "                    h5py.File(self.region_det_file_prefix+'_cls'+img_id[-self.id_digits:] +'.h5', 'r') as region_cls_f, \\\n",
    "                    h5py.File(bbox_file, 'r') as region_bbox_f:\n",
    "                img = torch.from_numpy(region_feat_f[img_id][:]).float()\n",
    "                cls_label = torch.from_numpy(region_cls_f[img_id][:]).float()\n",
    "                vis_pe = torch.from_numpy(region_bbox_f[img_id][:])\n",
    "        else:\n",
    "            # legacy, for some datasets, read data from numpy files\n",
    "            img = torch.from_numpy(np.load(img_path))\n",
    "            cls_label = torch.from_numpy(np.load(img_path.replace('.npy', '_cls_prob.npy')))\n",
    "            with h5py.File(self.region_bbox_file, 'r') as region_bbox_f:\n",
    "                vis_pe = torch.from_numpy(region_bbox_f[img_id][:])\n",
    "\n",
    "        # lazy normalization of the coordinates...\n",
    "        w_est = torch.max(vis_pe[:, [0, 2]])*1.+1e-5\n",
    "        h_est = torch.max(vis_pe[:, [1, 3]])*1.+1e-5\n",
    "        vis_pe[:, [0, 2]] /= w_est\n",
    "        vis_pe[:, [1, 3]] /= h_est\n",
    "        assert h_est > 0, 'should greater than 0! {}'.format(h_est)\n",
    "        assert w_est > 0, 'should greater than 0! {}'.format(w_est)\n",
    "        rel_area = (vis_pe[:, 3]-vis_pe[:, 1])*(vis_pe[:, 2]-vis_pe[:, 0])\n",
    "        rel_area.clamp_(0)\n",
    "\n",
    "        vis_pe = torch.cat((vis_pe[:, :4], rel_area.view(-1, 1), vis_pe[:, 5:]), -1) # confident score\n",
    "        normalized_coord = F.normalize(vis_pe.data[:, :5]-0.5, dim=-1)\n",
    "        vis_pe = torch.cat((F.layer_norm(vis_pe, [6]), \\\n",
    "            F.layer_norm(cls_label, [1601])), dim=-1) # 1601 hard coded...\n",
    "\n",
    "        \n",
    "        return (input_ids, segment_ids, input_mask, img, vis_masked_pos, vis_pe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pref = HATE_FEAT_PATH / 'feat_cls_1000/hateful_vlp_checkpoint_trainval'\n",
    "bbox_pref = HATE_FEAT_PATH / 'raw_bbox/hateful_vlp_checkpoint_trainval'\n",
    "id_digits=2\n",
    "\n",
    "truncate_config={\n",
    "    'max_len_b': args.max_tgt_length, 'trunc_seg': 'b', 'always_truncate_tail': True}\n",
    "\n",
    "max_masked = 10\n",
    "mask_prob = .2\n",
    "mask_img=True\n",
    "vis_mask_prob = 0.2\n",
    "train_proc = PreprocessVLP(max_masked, mask_prob,\n",
    "    list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, args.max_seq_length,\n",
    "    truncate_config=truncate_config,mask_image_regions=mask_img, vis_mask_prob=vis_mask_prob,\n",
    "    mode=\"bi\", len_vis_input=args.len_vis_input, \n",
    "    region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "    load_vqa_ann=True)\n",
    "\n",
    "val_proc = PreprocessVLP(0, 0,\n",
    "    list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, args.max_seq_length,\n",
    "    truncate_config=truncate_config,mask_image_regions=mask_img, vis_mask_prob=vis_mask_prob,\n",
    "    mode=\"bi\", len_vis_input=args.len_vis_input, \n",
    "    region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "    load_vqa_ann=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_do_batch(b):\n",
    "    if len(b[0]) == 2:\n",
    "        bx, by = zip(*b)\n",
    "        return VLPBatch.merge(bx), torch.tensor(by)\n",
    "    else:\n",
    "        return (VLPBatch.merge([x[0] for x in b]),)\n",
    "\n",
    "def get_y(row):\n",
    "    return int(row['label'])\n",
    "\n",
    "class LoadRow(Transform):\n",
    "   \n",
    "    def __init__(self, split_idx, processor):\n",
    "        self.split_idx = split_idx\n",
    "        self.proc = processor\n",
    "    \n",
    "    def encodes(self, x, **kwargs):\n",
    "        #print(self.proc.mask_prob)\n",
    "        return load_from_row(x, self.proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 500)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DataBlock(get_x = Pipeline([LoadRow(0, train_proc),LoadRow(1, val_proc)]), get_y=get_y,  splitter=ColSplitter('is_valid') )\n",
    "dls = db.dataloaders(data[:9000],bs=32, device=device, do_batch = my_do_batch)\n",
    "\n",
    "dls.n_inp=1\n",
    "len(dls.train_ds), len(dls.valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vlp_splitter(model):\n",
    "    return L(params(model.stem.vis_embed) + params(model.stem.vis_pe_embed), \n",
    "            params(model.stem.bert),\n",
    "            params(model.classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model()\n",
    "learn = Learner(dls, model,nn.CrossEntropyLoss(),metrics=[accuracy, RocAucBinary()], splitter=vlp_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     train_loss  valid_loss  accuracy  roc_auc_score  time    \n",
      "0         0.743977    0.738908    0.588000  0.618112       02:01     \n",
      "1         0.635486    0.760586    0.572000  0.622144       02:01     \n",
      "2         0.557166    0.788791    0.576000  0.688416       02:01     \n",
      "3         0.437160    0.729664    0.630000  0.715632       02:01     \n",
      "4         0.328169    0.872791    0.626000  0.711840       02:01     \n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, lr_max=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–ˆ\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dl_experiments/Hateful/util.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  _, indcs = preds.max(dim=1)\n"
     ]
    }
   ],
   "source": [
    "gen_submit(learn, 'attempts/attempt11.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlp",
   "language": "python",
   "name": "vlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
