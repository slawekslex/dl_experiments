{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pdb\n",
    "sys.path.insert(0, '/home/jupyter/VLP/pythia')\n",
    "sys.path.insert(0, '/home/jupyter/VLP/')\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTrainingLossMask\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from vlp.loader_utils import batch_list_to_batch_tensors\n",
    "import vlp.seq2seq_loader as seq2seq_loader\n",
    "from vlp.seq2seq_loader import truncate_tokens_pair\n",
    "import PIL\n",
    "from vlp.lang_utils import language_eval\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "import pythia.tasks.processors as pythia_proc\n",
    "\n",
    "from util import *\n",
    "from vlp_processor import PreprocessVLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_recover_path = '/home/jupyter/VLP/checkpoints/vqa2_g2_lr2e-5_batch512_ft_from_s0.75_b0.25/model.19.bin'\n",
    "model_recover = torch.load(model_recover_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    cls_num_labels = 2\n",
    "    type_vocab_size = 6\n",
    "    relax_projection = 4\n",
    "    task_idx_proj = 3\n",
    "    drop_prob = .2\n",
    "    max_alen = 100\n",
    "    max_blen = 100\n",
    "    model = BertForPreTrainingLossMask.from_pretrained(\n",
    "                    'bert-base-cased', state_dict=model_recover, num_labels=cls_num_labels,\n",
    "                    type_vocab_size=type_vocab_size, relax_projection=relax_projection,\n",
    "                    config_path=None, task_idx=task_idx_proj,\n",
    "                    max_position_embeddings=None, \n",
    "                    label_smoothing=0,\n",
    "                    fp32_embedding=False,\n",
    "                    cache_dir='./.pretrained_model',\n",
    "                    drop_prob=drop_prob, \n",
    "                    enable_butd=True,\n",
    "                    len_vis_input=max_alen,\n",
    "                    tasks='img2txt')\n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)\n",
    "\n",
    "max_seq_len = max_alen + max_blen + 3\n",
    "tokenizer.max_len = max_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred = 3\n",
    "mask_prob = .15\n",
    "vis_mask_prob=0\n",
    "mask_image_regions = vis_mask_prob > 0\n",
    "truncate_config={\n",
    "    'max_len_b': max_blen, 'trunc_seg': 'b', 'always_truncate_tail': True}\n",
    "region_pref = HATE_FEAT_PATH / 'feat_cls_1000/hateful_vlp_checkpoint_trainval'\n",
    "bbox_pref = HATE_FEAT_PATH / 'raw_bbox/hateful_vlp_checkpoint_trainval'\n",
    "id_digits=2\n",
    "proc=PreprocessVLP(max_pred, mask_prob,\n",
    "            list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, max_seq_len,\n",
    "            truncate_config=truncate_config, mask_image_regions=mask_image_regions,\n",
    "            mode=\"bi\", len_vis_input=max_alen,\n",
    "            vis_mask_prob=vis_mask_prob, \n",
    "            region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "             load_vqa_ann=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>don't be afraid to love again everyone is not like your ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i love everything and everybody! except for squirrels i hate squirrels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even hitler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>71352</td>\n",
       "      <td>img/71352.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fighting for gay rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2164</td>\n",
       "      <td>img/02164.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that feeling when you finish your homework in record time and have extra time to chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3587</td>\n",
       "      <td>img/03587.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the day that shook new york city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>47839</td>\n",
       "      <td>img/47839.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one of the first prototypes of the atom bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>34281</td>\n",
       "      <td>img/34281.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>today feels like a kinda day where i just wanna freak people out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12540 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            img  label  \\\n",
       "0     42953  img/42953.png    0.0   \n",
       "1     23058  img/23058.png    0.0   \n",
       "2     13894  img/13894.png    0.0   \n",
       "3     37408  img/37408.png    0.0   \n",
       "4     82403  img/82403.png    0.0   \n",
       "...     ...            ...    ...   \n",
       "1995  71352  img/71352.png    NaN   \n",
       "1996   2164  img/02164.png    NaN   \n",
       "1997   3587  img/03587.png    NaN   \n",
       "1998  47839  img/47839.png    NaN   \n",
       "1999  34281  img/34281.png    NaN   \n",
       "\n",
       "                                                                                        text  \n",
       "0                                           its their character not their color that matters  \n",
       "1                                 don't be afraid to love again everyone is not like your ex  \n",
       "2                                                                   putting bows on your pet  \n",
       "3                     i love everything and everybody! except for squirrels i hate squirrels  \n",
       "4                                        everybody loves chocolate chip cookies, even hitler  \n",
       "...                                                                                      ...  \n",
       "1995                                                                 fighting for gay rights  \n",
       "1996  that feeling when you finish your homework in record time and have extra time to chill  \n",
       "1997                                                        the day that shook new york city  \n",
       "1998                                            one of the first prototypes of the atom bomb  \n",
       "1999                        today feels like a kinda day where i just wanna freak people out  \n",
       "\n",
       "[12540 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PHASE_2 = Path('/home/jupyter/hate_phase2')\n",
    "train = pd.read_json(PHASE_2/'train.jsonl', lines=True)\n",
    "dev_seen = pd.read_json(PHASE_2/'dev_seen.jsonl', lines=True)\n",
    "dev_unseen = pd.read_json(PHASE_2/'dev_unseen.jsonl', lines=True)\n",
    "test_seen = pd.read_json(PHASE_2/'test_seen.jsonl', lines=True)\n",
    "test_unseen = pd.read_json(PHASE_2/'test_unseen.jsonl', lines=True)\n",
    "data = pd.concat([train, dev_seen, dev_unseen,test_seen,test_unseen])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>43976</td>\n",
       "      <td>img/43976.png</td>\n",
       "      <td>when your dive instructor starts yelling in arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            img                                                text\n",
       "656  43976  img/43976.png  when your dive instructor starts yelling in arabic"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_unseen[test_unseen.id==43976]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(row):\n",
    "    return 0\n",
    "def load_for_LM(row, proc, tokenizer, q=None):\n",
    "    img_id = row.id\n",
    "    text = row.text\n",
    "    if q is None: q = text\n",
    "    img_file = id_to_img_path(img_id)\n",
    "    instance = (img_file, tokenizer.tokenize(q), {'answers': ['dummy']})\n",
    "    input_ids, segment_ids, input_mask, lm_label_ids,   masked_pos, masked_weights, is_next, task_idx, conv_feats, vis_masked_pos, vis_pe, ans_labels= proc(instance)\n",
    "    \n",
    "    lm_label_ids, is_next, masked_pos,masked_weights, task_idx,vis_masked_pos = (torch.tensor(x) for x in [ lm_label_ids, is_next, masked_pos,masked_weights, task_idx,vis_masked_pos])\n",
    "    return conv_feats, vis_pe, input_ids, segment_ids, input_mask, lm_label_ids, ans_labels, is_next, masked_pos,masked_weights, task_idx,vis_masked_pos\n",
    "\n",
    "class LoadRow(Transform):\n",
    "   \n",
    "    def __init__(self,  processor, tokenizer):\n",
    "        self.proc = processor\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def encodes(self, x, **kwargs):\n",
    "        return load_for_LM(x, self.proc, self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = load_for_LM(data.iloc[idx], proc, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [x.unsqueeze(0) for x in inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBlock(blocks = (TransformBlock, CategoryBlock), \n",
    "               get_x = LoadRow(proc, tokenizer), \n",
    "               get_y=get_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10032, 2508)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = db.dataloaders(data,bs=32)\n",
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dls.one_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([32, 100, 2048]),\n",
       " torch.Size([32, 100, 1607]),\n",
       " torch.Size([32, 203]),\n",
       " torch.Size([32, 203]),\n",
       " torch.Size([32, 203, 203]),\n",
       " torch.Size([32, 3]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32, 3]),\n",
       " torch.Size([32, 3]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32, 0])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in batch[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_loss(x,y): return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting the new dropout rate! 0.2\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(dls, new_model(),loss_func=fake_loss).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.835008</td>\n",
       "      <td>3.257702</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.216209</td>\n",
       "      <td>2.939402</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.924983</td>\n",
       "      <td>2.686157</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.662813</td>\n",
       "      <td>2.582946</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.461544</td>\n",
       "      <td>2.444088</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.262028</td>\n",
       "      <td>2.342680</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.129048</td>\n",
       "      <td>2.330952</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.049532</td>\n",
       "      <td>2.274744</td>\n",
       "      <td>03:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(8 , lr_max = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/lm_p2.pth')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learn.save('trained_lm20drop')\n",
    "learn.save('lm_p2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateStem(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vlp):\n",
    "        super(HateStem, self).__init__()\n",
    "        self.vis_embed = vlp.vis_embed #Linear->ReLU->Linear->ReLU->dropout\n",
    "        self.vis_pe_embed = vlp.vis_pe_embed #Linear->ReLU->dropout\n",
    "        self.bert = vlp.bert # pytorch_pretrained_bert.modeling.BertModel\n",
    "        self.len_vis_input = vlp.len_vis_input\n",
    "        \n",
    "    \n",
    "    def forward(self, vis_feats, vis_pe, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        vis_feats = self.vis_embed(vis_feats) # image region features\n",
    "        vis_pe = self.vis_pe_embed(vis_pe) # image region positional encodings\n",
    "\n",
    "        sequence_output, pooled_output = self.bert(vis_feats, vis_pe, input_ids, token_type_ids,\n",
    "            attention_mask, output_all_encoded_layers=False, len_vis_input=self.len_vis_input)\n",
    "        #print(sequence_output.shape, pooled_output.shape)\n",
    "        vqa2_embed = sequence_output[:, 0]*sequence_output[:, self.len_vis_input+1]\n",
    "        return vqa2_embed\n",
    "        #return sequence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_stem = HateStem(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm_stem, 'checkpoints/lm_stem20drop_p2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlp",
   "language": "python",
   "name": "vlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
