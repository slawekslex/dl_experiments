{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b395c6348615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m  \u001b[0;31m# automatically select proper tqdm submodule if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/tqdm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm_gui\u001b[0m  \u001b[0;31m# TODO: remove in v5.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtgrange\u001b[0m  \u001b[0;31m# TODO: remove in v5.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tqdm_pandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m  \u001b[0;31m# TODO: remove in v5.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mComparable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_ascii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFormatReplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp_trim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mSimpleTextIOWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallbackIOWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_monitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# native libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/vlp/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pdb\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.insert(0, '/home/jupyter/VLP/pythia')\n",
    "sys.path.insert(0, '/home/jupyter/VLP/')\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTrainingLossMask\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from vlp.loader_utils import batch_list_to_batch_tensors\n",
    "import vlp.seq2seq_loader as seq2seq_loader\n",
    "import PIL\n",
    "from vlp.lang_utils import language_eval\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from vlp_processor import PreprocessVLP\n",
    "import pythia.tasks.processors as pythia_proc\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgDummy(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "args = ArgDummy()\n",
    "DATA_ROOT = '/mnt/ssd/data'\n",
    "HATE_FEAT_PATH = Path('/home/jupyter/hateful_features_p2/region_feat_gvd_wo_bgd')\n",
    "\n",
    "args['bert_model'] = 'bert-base-cased' #Bert pre-trained model selected\n",
    "args['seed'] = 123 #random seed for initialization\n",
    "args['len_vis_input'] = 100\n",
    "args['max_tgt_length'] = 100#20 #maximum length of target sequence\n",
    "args['region_det_file_prefix'] = 'feat_cls_1000/coco_detection_vg_100dets_gvd_checkpoint_trainval'\n",
    "args['output_dir'] ='tmp'\n",
    "args['drop_prob'] = 0.1\n",
    "args['model_recover_path'] = './checkpoints/vqa2_g2_lr2e-5_batch512_ft_from_s0.75_b0.25/model.19.bin'\n",
    "args['image_root'] = f'{DATA_ROOT}/flickr30k/region_feat_gvd_wo_bgd/'\n",
    "args['region_bbox_file'] =f'{DATA_ROOT}/flickr30k/region_feat_gvd_wo_bgd/flickr30k_detection_vg_thresh0.2_feat_gvd_checkpoint_trainvaltest.h5'\n",
    "args['do_lower_case'] = True\n",
    "args.region_bbox_file = os.path.join(args.image_root, args.region_bbox_file)\n",
    "args.region_det_file_prefix = os.path.join(args.image_root, args.region_det_file_prefix)\n",
    "args.max_seq_length = args.max_tgt_length + args.len_vis_input + 3 # +3 for 2x[SEP] and [CLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# fix random seed\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "tokenizer.max_len = args.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateStem(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vlp):\n",
    "        super(HateStem, self).__init__()\n",
    "        self.vis_embed = vlp.vis_embed #Linear->ReLU->Linear->ReLU->dropout\n",
    "        self.vis_pe_embed = vlp.vis_pe_embed #Linear->ReLU->dropout\n",
    "        self.bert = vlp.bert # pytorch_pretrained_bert.modeling.BertModel\n",
    "        self.len_vis_input = vlp.len_vis_input\n",
    "        \n",
    "    \n",
    "    def forward(self, vis_feats, vis_pe, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        vis_feats = self.vis_embed(vis_feats) # image region features\n",
    "        vis_pe = self.vis_pe_embed(vis_pe) # image region positional encodings\n",
    "\n",
    "        sequence_output, pooled_output = self.bert(vis_feats, vis_pe, input_ids, token_type_ids,\n",
    "            attention_mask, output_all_encoded_layers=False, len_vis_input=self.len_vis_input)\n",
    "        #print(sequence_output.shape, pooled_output.shape)\n",
    "        vqa2_embed = sequence_output[:, 0]*sequence_output[:, self.len_vis_input+1]\n",
    "        return vqa2_embed\n",
    "        #return sequence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head(nf, n_out, lin_ftrs=None, ps=0.5, bn_final=False, lin_first=False, ):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n",
    "    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "    ps = L(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    layers = []\n",
    "    layers = [Flatten()]\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateClassifier(torch.nn.Module):\n",
    "     def __init__(self, stem):\n",
    "        super(HateClassifier, self).__init__()\n",
    "        self.stem = stem\n",
    "        self.classifier = create_head(768,2, ps=.5)\n",
    "     #def forward(self, id, vis_feats, vis_pe, input_ids, token_type_ids, attention_mask):  \n",
    "     def forward(self, params):  \n",
    "        id, vis_feats, vis_pe, input_ids, token_type_ids, attention_mask = params\n",
    "        embs = self.stem(vis_feats, vis_pe, input_ids, token_type_ids, attention_mask)\n",
    "        return self.classifier(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    hate_stem = torch.load('checkpoints/lm_stem20drop.pth')\n",
    "    \n",
    "    return  HateClassifier(hate_stem).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE_2 = Path('/home/jupyter/hate_phase2')\n",
    "train = pd.read_json(PHASE_2/'train.jsonl', lines=True)\n",
    "dev_seen = pd.read_json(PHASE_2/'dev_seen.jsonl', lines=True)\n",
    "dev_unseen = pd.read_json(PHASE_2/'dev_unseen.jsonl', lines=True)\n",
    "test_seen = pd.read_json(PHASE_2/'test_seen.jsonl', lines=True)\n",
    "test_unseen = pd.read_json(PHASE_2/'test_unseen.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train, dev_seen, dev_unseen, test_seen, test_unseen])\n",
    "# data['label']=1\n",
    "# data.dtypes\n",
    "\n",
    "# df_train = pd.DataFrame(data[:10500])\n",
    "# df_valid = pd.DataFrame(data[10500:])\n",
    "\n",
    "# df_train['is_valid'] = False\n",
    "# df_valid['is_valid'] = True\n",
    "\n",
    "# valid_ids = df_valid.index.tolist()\n",
    "# valid_wrong = np.random.choice(valid_ids,len(valid_ids)//2)\n",
    "# valid_wrong[:5]\n",
    "\n",
    "# shuffled = np.random.permutation(valid_wrong)\n",
    "# shuffled[:5]\n",
    "\n",
    "# shuf_texts = df_valid.loc[shuffled]['text'].tolist()\n",
    "# shuf_texts[:5]\n",
    "\n",
    "# df_valid.loc[valid_wrong, 'text']= shuf_texts\n",
    "# df_valid.loc[valid_wrong, 'label'] =0\n",
    "\n",
    "# data = pd.concat([df_train, df_valid])\n",
    "# data.to_csv('data_sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_sim.csv')\n",
    "df_train = pd.DataFrame(data[:10500])\n",
    "df_valid = pd.DataFrame(data[10500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(idx, proc, q=None):\n",
    "    return load_from_row(data.iloc[idx],proc, tokenizer, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pref = HATE_FEAT_PATH / 'feat_cls_1000/hateful_vlp_checkpoint_trainval'\n",
    "bbox_pref = HATE_FEAT_PATH / 'raw_bbox/hateful_vlp_checkpoint_trainval'\n",
    "id_digits=2\n",
    "\n",
    "truncate_config={\n",
    "    'max_len_b': args.max_tgt_length, 'trunc_seg': 'b', 'always_truncate_tail': True}\n",
    "\n",
    "max_masked = 10\n",
    "mask_prob = .20\n",
    "mask_img=True\n",
    "vis_mask_prob = .20\n",
    "train_proc = PreprocessVLP(max_masked, mask_prob,\n",
    "    list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, args.max_seq_length,\n",
    "    truncate_config=truncate_config,mask_image_regions=mask_img, vis_mask_prob=vis_mask_prob,\n",
    "    mode=\"bi\", len_vis_input=args.len_vis_input, \n",
    "    region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "    load_vqa_ann=True)\n",
    "\n",
    "val_proc = PreprocessVLP(0, 0,\n",
    "    list(tokenizer.vocab.keys()), tokenizer.convert_tokens_to_ids, args.max_seq_length,\n",
    "    truncate_config=truncate_config,mask_image_regions=False, vis_mask_prob=0,\n",
    "    mode=\"bi\", len_vis_input=args.len_vis_input, \n",
    "    region_bbox_prefix=str(bbox_pref), region_det_file_prefix=str(region_pref), id_digits=id_digits,\n",
    "    load_vqa_ann=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typedispatch\n",
    "def show_batch(x:VLPInput, y, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    ctxs = show_batch[object](x, y, samples, ctxs=ctxs, max_n=max_n, **kwargs)\n",
    "    return ctxs\n",
    "\n",
    "@typedispatch\n",
    "def show_results(x:VLPInput, y:TensorCategory, samples, outs, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize)\n",
    "    for i in range(2):\n",
    "        ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    ctxs = [r.show(ctx=c, color='green' if b==r else 'red', **kwargs)\n",
    "            for b,r,c,_ in zip(samples.itemgot(1),outs.itemgot(0),ctxs,range(max_n))]\n",
    "    return ctxs\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def plot_top_losses(x: VLPInput, y:TensorCategory, samples, outs, raws, losses, nrows=None, ncols=None, figsize=None, **kwargs):\n",
    "    axs = get_grid(len(samples), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize, title='Prediction/Actual/Loss/Probability')\n",
    "    for ax,s,o,r,l in zip(axs, samples, outs, raws, losses):\n",
    "        s[0].show(ctx=ax, **kwargs)\n",
    "        ax.set_title(f'{o}/{s[1]} / {l.item():.2f} / {r.max().item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadRow(Transform):\n",
    "    \n",
    "    def __init__(self,processor, tokenizer, random_text=False, data = None):\n",
    "        self.proc = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_text = random_text\n",
    "        self.data = data\n",
    "    def encodes(self, x):\n",
    "        if self.random_text and random.choice((True, False)):\n",
    "            x = pd.Series(x)\n",
    "            altloc = random.randint(0, len(self.data)-1)\n",
    "            alttext = self.data.iloc[altloc].text\n",
    "            x['text'] = alttext\n",
    "            x['label'] = 0\n",
    "        return load_from_row(x, self.proc, self.tokenizer), x.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = LoadRow(train_proc, tokenizer, True, df_train)\n",
    "valid_load = LoadRow(val_proc, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tl = TfmdLists(df_train, train_load)\n",
    "valid_tl = TfmdLists(df_valid, valid_load)\n",
    "\n",
    "dls = DataLoaders.from_dsets(train_tl, valid_tl,bs=40, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@patch_to(VLPInput)\n",
    "def show(self, ctx, **kwargs):\n",
    "    id = self[0].item()\n",
    "    tit = id_to_text(id, data)\n",
    "    ctx.text(0,0,tit,ha='left', wrap=True)\n",
    "    ctx = show_image(PILImage.create(id_to_img_path(id)), ctx=ctx)\n",
    "    return ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vlp_splitter(model):\n",
    "    return L(params(model.stem.vis_embed) + params(model.stem.vis_pe_embed), \n",
    "            params(model.stem.bert),\n",
    "            params(model.classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model()\n",
    "learn = Learner(dls, model,metrics=[accuracy, RocAucBinary()], splitter=vlp_splitter, loss_func = nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max = 5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('sim_p2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "def show_by_idxs(img_idxs):\n",
    "    n,m = len(img_idxs)//2, 2\n",
    "    \n",
    "    _,axs = plt.subplots(n,m, figsize=(10*m,10*n))\n",
    "    for ax, idx in zip(axs.flatten(), img_idxs.view(-1)):\n",
    "        row = data.iloc[idx.item()]\n",
    "        img_path = PHASE_2 / row['img']\n",
    "        ax.imshow(PIL.Image.open(img_path))\n",
    "        ax.axis('off')\n",
    "        clr = 'red' if row['label']==1 else 'green'\n",
    "        txt = f'{row[\"id\"]}: {row[\"text\"][:20]}'\n",
    "        ax.set_title(txt, color=clr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_by_idxs(torch.tensor(range(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_row = data.iloc[0]\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# bs=64\n",
    "# all_probs = []\n",
    "# for batch_num in tqdm(range(len(data)//bs)):\n",
    "#     batch = []\n",
    "  \n",
    "#     for i in range(bs):\n",
    "#         txt = data.text.iloc[batch_num * bs + i]\n",
    "#         row_data = load_from_row(first_row, val_proc, tokenizer, txt)\n",
    "#         batch.append(row_data)\n",
    "#     b = fa_collate(batch)\n",
    "#     b = tuple([x.cuda() for x in b])\n",
    "#     with torch.no_grad():\n",
    "#         out = model(b)\n",
    "#     probs = F.softmax(out, dim=1)[:,1]\n",
    "#     all_probs.append(probs)\n",
    "\n",
    "# probs_tensor = torch.cat(all_probs)\n",
    "# probs_tensor.shape\n",
    "\n",
    "# probs, idxs = probs_tensor.topk(30)\n",
    "# probs\n",
    "\n",
    "# data.text.iloc[idxs.cpu().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImgRow(Transform):\n",
    "    \n",
    "    def __init__(self,processor, tokenizer):\n",
    "        self.proc = processor\n",
    "        self.tokenizer = tokenizer\n",
    "    def encodes(self, x):\n",
    "        return load_from_row(x, self.proc, self.tokenizer, '<SEP>')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tl = TfmdLists(data, LoadImgRow(val_proc, tokenizer))\n",
    "\n",
    "\n",
    "dls = DataLoaders.from_dsets(data_tl, valid_tl,bs=100, device=device, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_embs(data_batch):\n",
    "    feats =[]\n",
    "    for _, row in data_batch.iterrows():\n",
    "        bx = load_from_row(row, val_proc, tokenizer, \"<SEP>\")\n",
    "        vis_feats = bx[1]\n",
    "        feats.append(vis_feats.unsqueeze(0))\n",
    "    feats = torch.cat(feats).cuda()\n",
    "    print(feats.shape)\n",
    "    with torch.no_grad():\n",
    "        vis_embs = model.stem.vis_embed(feats)\n",
    "    return vis_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = []\n",
    "for b in tqdm(dls.train):\n",
    "    #with torch.no_grad():\n",
    "    #    vis_embs = model.stem.vis_embed(b)\n",
    "    vis_embs = b\n",
    "    all_embs.append(vis_embs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_v = torch.cat(all_embs,dim=0)\n",
    "embs_v = embs_v.view(embs_v.shape[0], -1)\n",
    "embs_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_cos(x, b):\n",
    "    return 1 -F.cosine_similarity(x.unsqueeze(0),b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = 1 -distances_cos(embs_v[8507], embs_v[:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.topk(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_by_idxs(dists.topk(6)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlp",
   "language": "python",
   "name": "vlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
