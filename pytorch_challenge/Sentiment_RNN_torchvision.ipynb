{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import datasets\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pytorch torchtext to load the training data and transform the text into numerical tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fields\n",
    "TEXT = data.Field(lower=True, include_lengths=False, batch_first=True,tokenize=\"spacy\", pad_first=True,truncate_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "#generate vocabulary on the train set\n",
    "TEXT.build_vocab(train, min_freq=10, vectors=vocab.GloVe(name='6B', dim=300))\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=100, device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a batch and decode it to see if everythings works as it shoud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" saints - especially the unbelievable character , al . i wonder if he 's got a job for me in <unk> ?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tensor_to_text(t, itos):\n",
    "    return \" \".join([itos[x] for x in t])\n",
    "\n",
    "batch = next(train_iter.__iter__())\n",
    "tensor_to_text(batch.text[0], TEXT.vocab.itos)[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_vect, hidden_dim, linear_dim = 128, n_layers=2, drop_prob=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        # initialize the embedding with pretrained word vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vect,freeze=True)\n",
    "        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, linear_dim)\n",
    "        self.fc2 = nn.Linear(linear_dim, 1)\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm1d(3 * hidden_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(linear_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds, None)\n",
    "    \n",
    "        # concatenate last element, mean and max of the lstm output sequence\n",
    "        lstm_last = lstm_out[:,-1,:] \n",
    "        lstm_mean = torch.mean(lstm_out, dim=1)\n",
    "        lstm_max,_ = torch.max(lstm_out, dim=1)\n",
    "        lstm_out = torch.cat([lstm_last, lstm_mean, lstm_max], dim=1)\n",
    "        \n",
    "        # dropout and fully-connected layers\n",
    "        out = self.dropout(F.relu(self.batchnorm1(lstm_out)))\n",
    "        out = self.dropout(F.relu(self.batchnorm2(self.fc1(out))))\n",
    "        out = self.fc2(out)\n",
    "        return torch.sigmoid(out)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "hidden_dim = 200\n",
    "n_layers = 2\n",
    "linear_dim = 512\n",
    "net = SentimentRNN(TEXT.vocab.vectors, hidden_dim, linear_dim, n_layers, drop_prob=.4).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for calculating model loss and accuracy on a given dataset\n",
    "def evaluate(net, data_iter, criterion):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        losses=[]\n",
    "        num_correct = 0\n",
    "        for batch in data_iter.__iter__():\n",
    "            inputs = batch.text\n",
    "            labels = batch.label - 1\n",
    "            output = net(inputs)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            #print(loss)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # convert output probabilities to predicted class (0 or 1)\n",
    "            pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "            # compare predictions to true label\n",
    "            correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "            correct =  np.squeeze(correct_tensor.cpu().numpy())\n",
    "            num_correct += np.sum(correct)\n",
    "    \n",
    "    # accuracy over all test data\n",
    "    acc = num_correct/len(data_iter.dataset)\n",
    "    return acc, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop function\n",
    "def train(epochs, lr):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,betas=(0.7, 0.99))\n",
    "\n",
    "    clip=1 # gradient clipping\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        net.train()\n",
    "        # batch loop\n",
    "        for batch in train_iter.__iter__():\n",
    "\n",
    "            inputs = batch.text\n",
    "            labels = batch.label - 1\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output= net(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss\n",
    "        total_loss /= len(train_iter)\n",
    "        # Get validation loss and accuracy\n",
    "        val_acc, val_loss = evaluate(net, test_iter, criterion)\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Loss: {:.6f}...\".format(total_loss.item()),\n",
    "              \"Val Loss: {:.6f}\".format(val_loss),\n",
    "              \"Val acc: {:.2f}\".format(val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Loss: 0.379463... Val Loss: 0.310542 Val acc: 0.87\n",
      "Epoch: 2/2... Loss: 0.286147... Val Loss: 0.282349 Val acc: 0.89\n"
     ]
    }
   ],
   "source": [
    "train(2, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Loss: 0.248861... Val Loss: 0.281046 Val acc: 0.89\n",
      "Epoch: 2/2... Loss: 0.243893... Val Loss: 0.277018 Val acc: 0.89\n"
     ]
    }
   ],
   "source": [
    "net.embedding.weight.requires_grad = True\n",
    "train(2,0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Loss: 0.240829... Val Loss: 0.279200 Val acc: 0.90\n"
     ]
    }
   ],
   "source": [
    "train(2,0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review\n",
    "\n",
    "You can change this test_review to any text that you want. Read it and think: is it pos or neg? Then see if your model predicts correctly!\n",
    "    \n",
    "> **Exercise:** Write a `predict` function that takes in a trained net, a plain text_review, and a sequence length, and prints out a custom statement for a positive or negative review!\n",
    "* You can use any functions that you've already defined or define any helper functions you want to complete `predict`, but it should just take in a trained net, a text review, and a sequence length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "# hard to interpret review\n",
    "test_review_hard = \"I have mixed feelings about this movie, there were parts I enjoyed and parts that I found too boring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, review):\n",
    "    net.eval()\n",
    "    tokenizer = spacy.blank('en').tokenizer\n",
    "    tokenized = [tok.text for tok in tokenizer(review)]\n",
    "    input_tensor = TEXT.numericalize([tokenized])\n",
    "    output = net(input_tensor.cuda())\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.000000\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "predict(net,test_review_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.837725\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "predict(net,test_review_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.958804\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "predict(net,test_review_hard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
